from selenium.webdriver.chrome.service import Service
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time


def get_flipkart_product_details(product_id):
    path = "C:/Users/Admin/PycharmProjects2/Web_scrapping/chromedriver-win64/chromedriver.exe"
    service = Service(path)
    driver = webdriver.Chrome(service=service)
    url = f"https://www.flipkart.com/product/p/itm?pid={product_id}"

    def scrape():
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "VU-ZEz"))
            )
            title = driver.find_element(By.CLASS_NAME, "VU-ZEz").text
            price = driver.find_element(By.CLASS_NAME, "Nx9bqj").text
            rating = driver.find_element(By.CLASS_NAME, "XQDdHH").text
            return {
                'title': title,
                'price': price,
                'rating': rating
            }
        except Exception as e:
            return {'error': str(e)}

    try:
        driver.get(url)
        print("Scraping first time...")
        first_scrape = scrape()

        print("Waiting for 15 minutes before refresh...")
        time.sleep(900)  # 15 minutes

        print("Refreshing and scraping again...")
        driver.refresh()
        second_scrape = scrape()

        result = {
            'first_scrape': first_scrape,
            'second_scrape': second_scrape
        }

    except Exception as e:
        result = {'error': str(e)}

    finally:
        driver.quit()

    return result
####################################################################################first used
from selenium.webdriver.chrome.service import Service
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time


def get_flipkart_product_details(product_id):
    path = "C:/Users/Admin/PycharmProjects2/Web_scrapping/chromedriver-win64/chromedriver.exe"
    service = Service(path)
    driver = webdriver.Chrome(service=service)
    url = f"https://www.flipkart.com/product/p/itm?pid={product_id}"

    try:
        driver.get(url)

        # Wait for product name to be visible
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "VU-ZEz"))
        )

        title_tag = driver.find_element(By.CLASS_NAME, "VU-ZEz").text
        price = driver.find_element(By.CLASS_NAME, "Nx9bqj").text
        rating = driver.find_element(By.CLASS_NAME, "XQDdHH").text

        result = {
            'title': title_tag,
            'price': f"{price}",
            'rating': rating
        }

    except Exception as e:
        result = {'error': str(e)}

    finally:
        driver.quit()

    return result


# # Run the function
# if __name__ == "__main__":
#     pid = input("Enter Flipkart Product ID (e.g., SNDFWGV3FZNKJEHW): ")
#     product_info = get_flipkart_product_details(pid)
#
#     print("\nProduct Info:")
#     for k, v in product_info.items():
#         print(f"{k.title()}: {v}")

##################################################################################### amazon
from flask import Flask, render_template, request
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

app = Flask(__name__)

def get_amazon_product_details(product_id):
    # Set Chrome options
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0")

    # Set ChromeDriver path (update this)
    service = Service(executable_path="C:/Users/Admin/PycharmProjects2/Web_scrapping/chromedriver-win64"
                                      "/chromedriver.exe")  # <-- CHANGE THIS
    driver = webdriver.Chrome(service=service, options=options)

    url = f"https://www.amazon.in/dp/{product_id}"

    try:
        driver.get(url)

        # Wait for product title to be present
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "productTitle"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        title_tag = soup.find(id='productTitle')
        price_tag = soup.find(class_='a-price-whole')
        rating_tag = soup.find('span', {'class': 'a-icon-alt'})

        title = title_tag.get_text(strip=True) if title_tag else 'Title not found'
        price = price_tag.get_text(strip=True) if price_tag else 'Price not found'
        rating = rating_tag.get_text(strip=True) if rating_tag else 'Rating not found'

        return {
            'title': title,
            'price': f"â‚¹ {price}",
            'rating': rating
        }

    except Exception as e:
        return {'error': str(e)}

    finally:
        driver.quit()

@app.route('/', methods=['GET', 'POST'])
def index():
    data = None
    if request.method == 'POST':
        product_id = request.form['product_id']
        data = get_amazon_product_details(product_id)
    return render_template('index.html', data=data)

if __name__ == '__main__':
    app.run(debug=True)
###################################################################################
from flask import Flask, render_template, request
from amazon_product import get_amazon_product_details
from flipkart_product import get_flipkart_product_details

app = Flask(__name__)

def choose_website(website_name, product_id):
    if website_name.lower() == "amazon":
        result = get_amazon_product_details(product_id)
    elif website_name.lower() == "flipkart":
        result = get_flipkart_product_details(product_id)
    else:
        result = {"error": "Unsupported website. Please enter either 'amazon' or 'flipkart'."}

    return result
    # for key, value in result.items():
    #     print(f"{key.title()}: {value}")
@app.route('/', methods=['GET', 'POST'])
def index():
    data = None
    if request.method == 'POST':
        product_id = request.form['product_id']
        data = get_amazon_product_details(product_id)
    return render_template('index.html', data=data)

if __name__ == '__main__':
    app.run(debug=True)

if __name__ == "__main__":
    website_name = input('Enter the website name (amazon/flipkart): ').strip()
    product_id = input('Enter the product ID: ').strip()
    choose_website(website_name, product_id)


####################################################################### new 
from flask import Flask, render_template, request, redirect
from amazon_product import get_amazon_product_details
from flipkart_product import get_flipkart_product_details
import threading
import time
import json
import os


app = Flask(__name__)

Database_File = 'Scraped_Data.json'

# ------------------- Persistent Storage -------------------
def load_Scraped_Data():
    if os.path.exists(Database_File):
        with open(Database_File, 'r') as file:
           return json.load(file)
    return {}

def saved_data(data):
    with open(Database_File, 'w') as file:
        json.dump(data, file)


scraped_data = load_Scraped_Data()


# ------------------- Background Scraper -------------------
def second_scrape_with_delayed(website_name, product_id):
    print(f"[INFO] 15 minutes delayed in second scrape and Product_Id :{product_id}")
    time.sleep(900) # 15 minutes delay

    if website_name.lower() == "amazon":
        _, second_scrape = get_amazon_product_details(product_id)
    elif website_name.lower() == 'flipkart':
        _, second_scrape = get_flipkart_product_details(product_id)
    else:
        second_scrape = {"error": "Unsupported website."}

    # Save second scrape under "second"
    if product_id in Scraped_Data:
        Scraped_Data[website_name][product_id]["second"] = second_scrape
    else:
        if website_name not in Scraped_Data:
            Scraped_Data[website_name] = {}
        Scraped_Data[website_name][product_id] = {"second": second_scrape}

    saved_data(Scraped_Data)
    print(f"[INFO] Second scrape completed and saved for: {product_id}")


# ------------------- Main Scraper Controller -------------------
def choose_correct_website(website_name, product_id):
    if website_name.lower() == "amazon":
        first_scrape, _ = get_amazon_product_details(product_id)
    elif website_name.lower() == "flipkart":
        first_scrape, _ = get_flipkart_product_details(product_id)
    else:
        first_scrape = {"error": "Unsupported website."}

    # Save first scrape under "first"
    # Scraped_Data[website_name] = product_id
    # Scraped_Data[product_id]= {"first": first_scrape}
    Scraped_Data[website_name][product_id] = {"first": first_scrape}
    saved_data(Scraped_Data)

    # Start second scrape in background
    thread = threading.Thread(target=second_scrape_with_delayed, args=(website_name, product_id))
    thread.start()


    return first_scrape


# ------------------- Routes -------------------
@app.route('/', methods=['GET', 'POST'])
def index():
    first_scrape = None
    second_scrape = None
    product_id = request.args.get('product_id')
    website_name = request.args.get('website_name')

    if request.method == 'POST':
        website_name = request.form['website_name']
        product_id = request.form['product_id']
        first_scrape = choose_correct_website(website_name, product_id)
        return redirect(f"/?product_id={product_id}&website_name={website_name}")

    if product_id:
        data = load_Scraped_Data().get(product_id, {})
        first_scrape = data.get("first")
        second_scrape = data.get("second")

    return render_template('index.html', first_scrape= first_scrape,
                           second_scrape = second_scrape, product_id = product_id, website_name = website_name)

if __name__ == '__main__':
    app.run(debug=True)
